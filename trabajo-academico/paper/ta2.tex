\RequirePackage[l2tabu, orthodox]{nag}
% The following is to avoid the incompatibility between the nag package and babel.
% See https://tex.stackexchange.com/questions/240868/how-to-write-cases-with-latex
\documentclass[journal]{IEEEtran}

\usepackage[utf8]{inputenc}
\usepackage{comment}
\usepackage[english, spanish, es-lcroman]{babel}


\PassOptionsToPackage{usenames,dvipsnames,svgnames}{xcolor}  

\usepackage{cite}
\usepackage[pdftex]{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\interdisplaylinepenalty=2500
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{array}
\usepackage[binary-units=true]{siunitx}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{microtype}
\usepackage{tikz}
\usepackage{tkz-graph}
\usetikzlibrary{arrows,positioning,automata}
\usepackage{pgfplots,pgfplotstable}

\ifCLASSOPTIONcompsoc
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\else
\usepackage[caption=false,font=footnotesize]{subfig}
\fi

\ifCLASSOPTIONcaptionsoff
\usepackage[nomarkers]{endfloat}
\let\MYoriglatexcaption\caption
\renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
\fi

\let\MYorigsubfloat\subfloat
\renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}

\usepackage{url}
\usepackage{lipsum}

\makeatletter
\renewcommand{\ALG@name}{Algoritmo}
\renewcommand{\listalgorithmname}{Lista de \ALG@name s}
\makeatother

\renewcommand\IEEEkeywordsname{Palabras clave}
\newcommand\mydots{\hbox to 1em{.\hss.\hss.}}

\renewcommand{\algorithmicend}{\textbf{fin}}
\renewcommand{\algorithmicif}{\textbf{si}}
\renewcommand{\algorithmicthen}{\textbf{luego}}
\renewcommand{\algorithmicfunction}{\textbf{función}}
\renewcommand{\algorithmicwhile}{\textbf{mientras}}
\renewcommand{\algorithmicrepeat}{\textbf{repetir}}
\renewcommand{\algorithmicdo}{\textbf{hacer}}
\renewcommand{\algorithmicfor}{\textbf{para}}
\renewcommand{\algorithmicreturn}{\textbf{devolver}}
\renewcommand{\algorithmicelse}{\textbf{caso contrario}}

\newcommand{\Continue}{\State \textbf{continuar} }
\algdef{SE}[DOWHILE]{Do}{doWhile}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%

\newcommand{\matr}[1]{\mathbf{#1}} % undergraduate algebra version
%\newcommand{\matr}[1]{#1}          % pure math version
%\newcommand{\matr}[1]{\bm{#1}}     % ISO complying version
\newcommand{\pkg}[1]{\texttt{#1}}

\pgfplotsset{compat=1.14}

\begin{document}
\title{Una aplicación de métricas de centralidad a redes de coautoría}
\author{Kevin~Languasco,~\IEEEmembership{20132102}
        Melany~Chávez,~\IEEEmembership{20092344}
        y~Jesús~Advíncula,~\IEEEmembership{20135452}%
}

\maketitle
\begin{abstract}
Presentamos las métricas de centralidad Degree, Closeness y PageRank para grafos no dirigidos con pesos, y los algoritmos para calcularlos. En particular, se estudia el algoritmo de Dijkstra para hallar distancias más cortas.

Como aplicación, se consideran tres redes de coautoría del DBLP\@. A través de una implementación en C, se calculan los tiempos de ejecución de cada algoritmo, y se hace una comparación. Consideramos también la distancia de colaboración con respecto al nodo más importante según cada métrica, y el método para hallarlo. Para el algoritmo de PageRank, se deduce experimentalmente la influencia de dos parámetros en la cantidad de iteraciones: el factor de amortiguamiento y la precisión; y se conjetura un modelo log-lineal para el último.
\end{abstract}
\begin{IEEEkeywords}
PageRank, grafos, centralidad, coautoría
\end{IEEEkeywords}
\IEEEpeerreviewmaketitle

\section{Introducción}
\IEEEPARstart{U}{na} métrica de centralidad sobre un grafo asigna un número a cada vértice, midiendo la importancia de éste. La idea intuitiva de la importancia de un vértice no está definida de manera única, por lo que se tienen diferentes métricas, cada una trabajando con un concepto diferente. \cite{brandes}.

El problema de encontrar vértices centrales tiene sus orígenes en la Sociología, donde el grafo representa una red social, y las aristas son las conexiones entre los miembros del grupo. Un nodo central se interpreta, entonces, como la persona más influyente de la red.

Las redes de coautoría son grafos donde cada nodo representa un autor, y las aristas entre dos autores tienen asignadas un peso, interpretado como la cantidad de artículos coautorados.

En este artículo, describimos tres métricas de centralidad: \textit{Closeness}, \textit{Degree} y \textit{PageRank}; y los algoritmos para calcularlos en cada caso. Como aplicación, empleamos cada algoritmo a redes de coautoría obtenidas del DBLP.

\section{Descripción de métricas} \label{sec:metricas}

Definimos, como es usual, un \textbf{grafo} como un par \(G = (V, E)\), donde \(V\) es el conjunto de vértices o nodos, y \(E \subset [V]^2 \) el de aristas, con \([V]^2 = \{ X \subset V : |X| = 2 \}\). Una consecuencia de esta definición es que el grafo debe ser no dirigido y las aristas están completamente determinadas por los nodos que unen \cite{diestel}. Escribimos \(uv\) en lugar de \(\{u, v\}\) por facilidad notacional (nótese que \(uv = vu\)).

Consideramos una función \(w: E \rightarrow \mathbb{N} \) que asigna a cada arista un \textbf{peso}. Esto nos permite hablar de una relación múltiple entre dos nodos (e.g.\ múltiples colaboraciones entre dos autores)\footnote{La descripción usual para trabajar con aristas múltiples es la de multigrafo \cite{diestel}, pero preferimos emplear una función auxiliar para tener una representación más sencilla de implementar. }.
\subsection{Degree}
Para cada vértice \(v\), sea \(\Gamma(v) = \{u \in V : uv \in E \}\) la \textbf{vecindad} de \(v\). Decimos que \(u\) es \textbf{adyacente} a \(v\) si \(u \in \Gamma(v)\). Claramente esta relación es simétrica.

Sea \(E(v) = \{uv \in E : u \in \Gamma(v) \}\). Si \(e \in E(v)\), decimos que el vértice \(v\) es \textbf{incidente} a la arista \(e\).

El \textbf{grado} de \(v\) es la cantidad de aristas incidentes con él. En nuestro caso, consideramos múltiples aristas para un par de nodos a través de la función \(w\), por lo que escribimos \cite{bollobas}
\begin{equation}
	deg(v) = \sum_{e \in E(v)} w(e)
\end{equation}
La \textbf{centralidad de grado} para un vértice es simplemente \(C_D (v) = deg(v)\).
\subsection{Closeness}
Un \textbf{camino} en un grafo es una secuencia \(p = (x_1, x_2, \ldots x_n)\) de vértices tales que \(x_{i}x_{i+1} \in E\) para \(1 \leq i < n\), y su longitud es \(|p| = n\). Decimos que un camino une dos vértices \(u, v\) si \(x_1 = u\) y \(x_n = v\). Denotamos por \(P(u, v)\) al conjunto de todos los caminos que unen a \(u\) con \(v\). Sea \(c(uv) = w(uv)^{-1}\) el \textbf{costo} de la arista \(uv\). La \textbf{distancia} entre dos vértices es el resultado del siguiente problema de optimización:
\begin{equation} \label{eq:distance}
d(u, v) = \min \left(\sum_{i=1}^{n-1} c(x_{i}x_{i+1}) : (x_1, \mydots x_n) \in P(u, v)\right)
\end{equation}
Por conveniencia, definimos \(d(u, u) = 0\). Si no existe camino entre \(u\) y \(v\), i.e. \(P(u, v) = \varnothing\), se dice que \(d(u, v) = \infty\) \cite{bollobas}. La idea del costo es que, a mayor cantidad de artículos coautorados, hay más cercanía entre autores, y por ello el camino entre ellos debe ``costar menos'' \cite{newman}.

Un grafo es \textbf{conexo} si \(P(u, v) \neq \varnothing~\forall u, v \in V, u \neq v\).

La medida de \textbf{cercanía} para un nodo es \cite{brandes}:
\begin{equation} \label{eq:close}
C_C (u) = \frac{1}{\sum_{v \in V} d(u, v)}
\end{equation}

Algo que se debe tomar en cuenta es que, si no se tiene conexidad del grafo, la suma anterior no va a tener sentido. Por esto, para grafos disconexos se usa una medida parecida, definida como
\begin{equation}
C_H (u) = \sum_{v \in V \setminus \{u\}} \frac{1}{d(u, v)}
\end{equation}
Esta medida es algunas veces llamada la métrica \textbf{armónica} de centralidad\cite{rochat}. Asumimos que \(1/\infty = 0\).
\subsection{PageRank}
Sea \(\alpha\) un número en \([0, 1]\), llamado el \textbf{factor amortiguador}.

Definimos el \textbf{PageRank} de un vértice de manera recursiva:

\begin{equation} \label{eq:pagerank}
	PR(u) = \frac{(1-\alpha)}{|V|} + \alpha \sum_{v \in \Gamma(u)} \frac{PR(v)}{deg(v)} w(uv)
\end{equation}

Llamaremos a cada término de la sumatoria el \textit{aporte del vértice} \(v\) \textit{al vértice} \(u\).

PageRank fue originalmente diseñado para cuantificar la importancia de un sitio web, con el fin de implementar un sistema de búsqueda. La definición se basa en la idea de una persona a la que se le asigna un sitio web aleatorio, y va moviéndose a otros sitios a través de hipervínculos. El factor amortiguador es la probabilidad de que la persona deje de hacer este procedimiento y pida otro sitio aleatorio. El PageRank de un sitio web representa la probabilidad de que la persona ingrese al mismo \cite{google}. El aporte a cada sitio a donde se apunta se distribuye equitativamente.

La definición original se aplica a grafos dirigidos sin aristas múltiples. En nuestro caso, no hace falta distinguir entre hipervínculos que salen del sitio web, y los que van hacia él. Sin embargo, sí es importante tomar en cuenta los pesos asignados a cada arista. Si un autor colabora más de una vez con una persona importante, esto debe reflejarse en el PageRank asignado. Por eso, el aporte ya no es equitativo entre los vecinos de un nodo, pero sí entre sus aristas. Para considerar este efecto, se pone el factor del peso en el aporte.

El factor de amortiguamiento es importante para grafos disconexos, pues tomando \(\alpha = 1\), si se empieza en un vértice cualquiera, la probabilidad de llegar a los de una componente conexa diferente va a ser nula. Por otro lado, si \(\alpha = 0\), el viaje será completamente aleatorio. La calibración de este parámetro determina, entonces, en qué medida la estructura propia del grafo influencia en la asignación de importancia. En la literatura original se trabajó con \(\alpha = 0.85\) \cite{google}, pero resulta interesante variar este parámetro por su efecto en la tasa de convergencia\cite{stanford}.

Cada métrica determina un orden de vértices según el valor asignado. El vértice que maximiza el valor de una métrica \(C\) es llamado el \textbf{Ërdos} de \(G\) según \(C\). Si \(v_e\) es el Ërdos de \(G\), el \textbf{número de Ërdos} para cualquier \(v \in V\) se define como la distancia entre \(v_e\) y \(v\) considerando \(w(e) = 1\) para todo \(e \in E\).

\section{Algoritmos y estructuras de datos}

Una \textbf{cola de prioridad} es una estructura de datos que contiene pares de elementos, \(P = \{(x_l, k_l)\}_{l \in L}\), donde la segunda componente \(k_l\) es llamada la \textbf{llave} de \(x_l\). Debe tener las siguientes operaciones\footnote{También se pide que se pueda disminuir el valor de la llave de un elemento dado, pero no usaremos esta característica en el presente trabajo para disminuir la complejidad.} (El dual sería trabajar con los máximos en lugar de los mínimos) \cite{clrs}

\begin{itemize}
	\item \(P.sacar\_min()\) devuelve el elemento con menor llave y lo saca de la estructura.
	\item \(P.agregar((x, k))\) coloca un nuevo par al conjunto.
\end{itemize}

Escribiremos \((x_1, k_1) < (x_2, k_2)\) cuando \(k_1 < k_2\).

Hay varias maneras de implementar una cola de prioridad. La que teóricamente presenta menor complejidad computacional es el \textit{montículo de Fibonacci}, que fue explícitamente creado para problemas en la categoría que nos interesa. Sin embargo, presenta dos grandes desventajas. En primer lugar, es muy complejo de implementar, pues requiere trabajar con varios árboles disjuntos, para luego unirlos y transformarlos siguiendo ciertas reglas. Por otro lado, se sabe que en la práctica resulta lento: Las constantes detrás de su baja complejidad son grandes, y requieren de más memoria que otras implementaciones \cite{heap}.

La implementación que usamos es la de \textbf{montículo binario}. Un montículo, por definición, es un árbol que mantiene la siguiente propiedad: El padre debe tener una llave mayor que la de sus hijos. Un montículo binario se basa en mantener el árbol balanceado agregando nodos de izquierda a derecha en cada nivel. Esto permite almacenar el árbol en un arreglo, y acceder a los diferentes elementos de manera eficiente a través de operaciones con los índices.

Sea \(H = [(x_1, k_1),\ldots,(x_n, k_n)]\) el arreglo de pares que representa a la cola de prioridad \(P\). Para todo elemento de \(P\), al cual le corresponde un índice \(i\) de \(H\), se tendrá que el hijo izquierdo, de existir, estará indexado por \(2i\), y el hijo derecho por \(2i + 1\). El padre estará en la posición \( \lfloor i/2 \rfloor \). El problema de encontrar el mínimo de \(P\) se reduce entonces a sacar el elemento \(H[0]\) del arreglo, y la posición de inserción de un nuevo elemento siempre será \(H[n+1]\), al final del arreglo. Sin embargo, debemos mantener la condición de montículo para que el mínimo siempre esté en la raíz. Con este fin, en cada inserción y extracción del mínimo se debe emplear un algoritmo restaurador, como el mostrado en la función \textproc{Heapify}. Como consecuencia, las operaciones antes mencionadas son de complejidad \( \mathcal{O}(\log n) \) \cite{clrs}.

\begin{algorithm}
	\caption{Restauración de condición de montículo} \label{alg:heapify}
	\begin{algorithmic}
		\Function{Heapify}{$H$, $i$}
		\State Sea $n$ la longitud de $H$
		\State $l \gets 2\times i$
		\State $r \gets 2\times i + 1$
		\State $menor \gets i$ \Comment{En principio, el menor elemento es el que tiene índice \(i\) por ser padre.}
		\If{$l \leq n~\&~H[l] < H[i]$}
		\State $menor \gets l$
		\EndIf
		\If{$r \leq n~\&~H[r] < H[i]$}
		\State $menor \gets r$
		\EndIf
		\If{$menor \neq i$} \Comment{Si no son iguales, algún hijo es menor que el padre.}
		\State $Intercambiar(H[i], H[menor])$
		\State \textsc{Heapify}($H$, $menor$)
		\EndIf
		\EndFunction
	\end{algorithmic}
\end{algorithm}

Para verificar si un grafo es conexo, se puede hacer una \textbf{búsqueda en anchura}, marcando como visitados todos los nodos por los que se va pasando, y agregando los adyacentes aún no visitados a una cola. Al final del recorrido, se debe haber visitado el grafo completo. Caso contrario, el grafo es disconexo. También es posible usar una búsqueda en profundidad, pues la idea es similar.

\begin{algorithm}[t]
	\caption{Búsqueda en anchura para probar conexidad} \label{alg:connected}
	\begin{algorithmic}
		\Function{EsConexo}{$Grafo$, $inicio$}
		\State Sea $A$ un arreglo de ceros (no visitados aún).
		\State Sea $Q$ una cola.
		\State $A[inicio] \gets 1$
		\State $Q.agregar(inicio)$
		\While{$Q$ no esté vacío}
		\State $u \gets Q.descolar()$
		\For{$v \in \Gamma(u) : A[v] = 0$}
		\State $A[v] \gets 1$
		\State $Q.insertar(v)$
		\EndFor
		\EndWhile
		\If{$\sum_{v \in V} A[v] = |V|$}
		\Comment Se han visitado todos los nodos.
		\State \Return Verdadero
		\Else
		\State \Return Falso
		\EndIf
		\EndFunction
	\end{algorithmic}
\end{algorithm}

En lo que resta del capítulo veremos algoritmos para calcular las métricas ya descritas.

La métrica del grado se calcula de manera directa: Para cada nodo, sumamos los pesos de todas las aristas incidentes a él.

La medida de cercanía involucra resolver el problema de minimización planteado en la Ecuación~\ref{eq:distance}. Este es un problema conocido, y se puede resolver de varias formas.

Ya que se necesita calcular el camino más corto entre todo par de nodos en el grafo, se puede aplicar el algoritmo de Floyd-Warshall, con complejidad \(\Theta(|V|^3)\) \cite{clrs}. Sin embargo, si el grafo es esparso\footnote{Se considera esparso cuando \(|E| = o(|V|^2 / \log(|V|))\)\cite{clrs}}, se puede considerar aplicar el algoritmo de Dijkstra para todo vértice, con complejidad \(\mathcal{O}(|V||E|\ln |V|)\) usando colas de prioridad (para evitar hacer una búsqueda lineal del mínimo en cada paso). Esta alternativa es la que se usó en la presente investigación, al ser relativamente de baja complejidad y más eficiente para el caso particular de interés.

La función \textproc{Dijkstra} usa un arreglo de distancias mínimas, donde todas están inicializadas a \(\infty\), con excepción del nodo raíz, que tiene distancia \(0\) hacia sí mismo. La cola de prioridad usada \(P\) está compuesta por 2-tuplas \((d, u)\), donde \(d\) es la distancia desde el nodo raíz hasta \(u\). Cada vez que saquemos el nodo con mínima distancia \(u\) de \(P\), se compara, para cada vértice adyacente \(v\), la distancia si se sigue por \(u\) hasta \(v\), y la hipotética distancia mínima actual para \(v\). Si la distancia efectivamente disminuye, actualizamos el arreglo de distancias mínimas. En este punto, una técnica usual es disminuir la prioridad de \(v\) en \(P\)\cite{heap}, pero esto implicaría reordenar la estructura. Optamos por agregar \((nuevo, v)\) a \(P\), donde \(nuevo\) es la nueva distancia mínima hallada. Nótese entonces que cada nodo \(u\) puede aparecer más de una vez, pero en este caso sólo sale de la cola, sin hacer más computación. Para hallar la métrica de cercanía, se aplica la Ecuación~\ref{eq:close} sobre el arreglo \(dist\) resultante.
\begin{algorithm}[t]
	\caption{Algoritmo de Dijkstra} \label{alg:dijkstra}
\begin{algorithmic}
	\Function{Dijkstra}{$Grafo$, $fuente$}
		\State $dist[fuente] \gets 0$
		\State $P.agregar((0, fuente))$
		\For{$u \in Grafo, u \neq fuente$}
			\State $dist[u] = \infty$
		\EndFor
		\While {$P$ no está vacío}
			\State $(d, u) \gets P.sacar\_min()$
			\If{$d <= dist[u]$}
				\For{$uv \in E(u)$}
					\State $peso \gets 1/w(uv)$
					\State $nuevo \gets peso + dist[u]$
					\If{$nuevo < dist[v]$}
						\State $dist[v] = nuevo$
						\State $P.agregar((nuevo, v))$
					\EndIf
				\EndFor
			\EndIf
		\EndWhile
	\EndFunction
\end{algorithmic}
\end{algorithm}

PageRank puede ser interpretado de la siguiente manera. Consideremos la matriz \(\matr{H} = [H_{ij}]\) dada por:
\begin{equation}
	H_{ij} =
	\begin{dcases}
	\frac{w(v_i v_j)}{deg(v_j)} &\text{si $v_i v_j \in E$}\\
	0\phantom{\frac{1}{|V|}} &\text{caso contrario}
	\end{dcases}
\end{equation}
Esta matriz tiene la propiedad de que la suma de sus columnas es \(1\), y ninguna entrada es negativa. En la literatura, una matriz con esta propiedad es llamada \textbf{matriz estocástica izquierda}. En los trabajos originales sobre PageRank, era de interés calcular la importancia de sitios web sin hipervínculos, en cuyo caso la matriz anterior requiere de una pequeña modificación \cite{google}. En nuestro caso, esto no es necesario ya que en las redes de coautoría sólo se trabajan con autores que han publicado conjuntamente con otros autores, haciendo que no puedan existir vértices aislados.

Cada entrada se interpreta como la probabilidad de llegar al vértice \(v_i\) inmediatamente a partir del \(v_j\). Como ya se mencionó en el Capítulo~\ref{sec:metricas}, es importante que se tome un valor razonable de \(\alpha\). Con este fin, introducimos una nueva matriz \(\matr{J}\) tal que todas sus entradas son la unidad.

La \textbf{matriz de Google} será \cite{langville}
\begin{equation}
	\matr{G} = \frac{1-\alpha}{|V|} \matr{J} + \alpha \matr{H}
\end{equation}
Por convexidad, \(\matr{G}\) también es estocástica izquierda. El problema de resolver la Ecuación~\ref{eq:pagerank} simultáneamente para todos los vértices es equivalente a encontrar algún \(\bm{\pi} \in \mathbf{R}^{|V|} \) tal que
\begin{equation}
	\matr{G} \bm{\pi} = \bm{\pi}
\end{equation}
En otras palabras, nos interesa encontrar un eigenvector asociado al eigenvalor \(\lambda = 1\). Es un resultado conocido que este eigenvector existe y es único, y que \(1\) es el máximo valor absoluto posible para eigenvalores de matrices estocásticas\cite{meyer}.

El método para resolver este problema, conocido como el \textbf{método de potencias}, consiste en aplicaciones iteradas de la matriz \(\matr{G}\) a un vector \(\bm{\pi}_0\) inicial. La demostración de convergencia consiste en descomponer \(\matr{G}\) en bloques de Jordan y aplicar el resultado mencionado en el párrafo anterior\cite{langville}.

La elección del vector inicial \(\bm{\pi}_0\) influye en la velocidad de convergencia del algoritmo. Es usual tomar \(\bm{\pi}_0 = \frac{1}{|V|} \bm{1} \), donde \(\bm{1}\) tiene cada componente igual a \(1\). Otra alternativa es inicializar el vector con los valores normalizados de las dos métricas anteriores, esperando que se esté más cerca al eigenvector deseado.

La función \textproc{PageRank} aplica el método de potencias hasta que la diferencia máxima entre los valores antes y después de la iteración sea menor que \(\epsilon\). Naturalmente, la cantidad de iteraciones necesarias para que se cumpla la condición de convergencia crecerá al disminuir \(\epsilon\).

\begin{algorithm}
	\caption{PageRank} \label{alg:pagerank}
	\begin{algorithmic}
		\Function{PageRank}{$Grafo$, $\epsilon$, $\alpha$}
			\State Inicializar los vértices
			\Do
				\State $\Delta \gets 0$
				\For{$u \in Grafo$}
					\State $PR_0 \gets PR(u)$
					\State $PR_1 \gets 0$
					\For{$uv \in E(u)$}
						\State $PR_1 \gets PR_1 + (PR(v) \times w(uv))/deg(v)$
					\EndFor
					\State $PR_1 \gets PR_1 \times \alpha$
					\State $PR_1 \gets PR_1 + (1-\alpha)/|V|$
					\State $PR(u) \gets PR_1$
					\State $\Delta \gets max(\Delta, |PR_1 - PR_0|)$
				\EndFor
			\doWhile{$\Delta > \epsilon$}
		\EndFunction
	\end{algorithmic}
\end{algorithm}

\section{Experimentos y resultados}
Se tienen tres conjuntos de datos sobre los cuales se trabajó. Los tres son parte de una red de coautoría obtenida del DBLP, cuyo grafo resultante contiene un total de \num{1411345} vértices y \num{5946615} aristas, sin contar sus pesos (contándolos, hay \num{10615809}). Este grafo será etiquetado Grafo 3. Los otros dos grupos de datos se etiquetarán Grafo 1 (con \num{1650} vértices y \num{31075} aristas) y Grafo 2 (\num{11454} vértices, \num{231482} aristas).

Los grafos fueron representados como \textbf{listas de adyacencia}: Para cada vértice, se tiene una lista simple de aristas, cada una conteniendo el nodo al cual se dirige y su peso. Los vértices están indexados en un arreglo para poder acceder rápidamente a ellos. Ya que los grafos dados son no dirigidos, se agregó una arista para ambos vértices incidentes a la arista teórica.

La lectura de los grafos involucró leer pares de nombres, representando aristas entre dos nodos. Agregar cada arista involucra comparar los nombres con cada uno de los ya colocados en el grafo. Esto puede demorar bastante para grafos grandes, por lo que se creó un Árbol Binario de Búsqueda auxiliar de manera que conseguir el índice del vértice correspondiente a cada nombre sea de complejidad \(\mathcal{O}(\log(|V|))\)\footnote{Esta complejidad se consigue balanceando los árboles, lo cual no se hizo en este caso. Sin embargo, la profundidad del árbol no balanceado creado para el Grafo 3 fue 55, un número bastante razonable.}.

\begin{table}[t]
	\renewcommand{\arraystretch}{1.3}
	\caption{Tiempos de ejecución: Degree y Closeness}
	\label{tab:tejecucion}
	\centering
	\begin{tabular}{c|c|c}
		\hline
		Grafo & \(C_D\) (s) & \(C_C\) (s) \\
		\hline\hline
		1 & 0.0000 (0.000165) & 1.13199 (0.194443) \\
		2 & 0.0204 (0.002393) & 85.3177 (11.288503) \\
		3 & 0.5938 (0.592077) & - \\
		\hline
	\end{tabular}
\end{table}

\begin{table}[t]
	\renewcommand{\arraystretch}{1.3}
	\caption{Tiempos de ejecución: PageRank}
	\label{tab:tpagerank}
	\centering
	\begin{tabular}{c|c|c}
		\hline
		Grafo & Por Iteración (s) & Total (\( \epsilon = 10^{-13}, \alpha = 0.85 \)) (s) \\
		\hline\hline
		1 & 0.0002 (0.000173) & 0.01389 (0.013492) \\
		2 & 0.0021 (0.002064) & 0.15160 (0.150690) \\
		3 & 0.6490 (0.649002) & 56.4630 (56.46315) \\
		\hline
	\end{tabular}
\end{table}

Los Grafos 1 y 2 son conexos, así que no hay problema aplicando la métrica de cercanía en estos casos. El Grafo 3, por otro lado, es disconexo, así que tendremos que usar la métrica armónica para tener resultados útiles.

Para los siguientes experimentos, se ha usado una computadora con procesador Intel i7-3770k, \SI{16}{\giga\byte} de memoria RAM y Windows 10. Todas las estructuras de datos y los algoritmos fueron implementados en C. Para la compilación, se usó \pkg{GNU Compiler Collection (GCC)} (versión 6.3.0) con nivel de optimización \texttt{-O3}.

En lo que sigue, diferenciaremos entre \textit{tiempo de CPU}, que es el tiempo que el CPU dedica al proceso, sin considerar lo que se dedica a los demás procesos; y el \textit{tiempo real} (también conocido como \textit{wallclock time}), que es lo que demoró el proceso en ejecutar, como se entiende de manera cotidiana. Por claridad, escribiremos el tiempo real entre paréntesis luego de escribir el tiempo de CPU.

La aplicación de \textproc{Dijkstra} a un nodo es independiente de su aplicación al resto. Es por ello que se puede hacer una paralelización de manera trivial, y obtener resultados más rápidamente. Con este fin, empleamos \pkg{OpenMP}, una API que extiende C a través de directivas al compilador, permitiendo insertar secciones de código a ser ejecutadas en paralelo \cite{openmp}. El compilador escogido, \pkg{GCC}, da soporte a \pkg{OpenMP} de manera nativa. Todos los tiempos para la métrica de cercanía fueron obtenidos usando 8 hilos de ejecución.

En el Cuadro~\ref{tab:tejecucion} se aprecian los tiempos (de CPU y reales) para los algoritmos que evalúan las métricas Degree y Closeness, y en el Cuadro~\ref{tab:tpagerank} mostramos el tiempo que tarda una iteración del algoritmo de PageRank, junto a lo que se tomó en ejecutar en total para \( \epsilon = 10^{-13}\). Los tiempos presentados son un promedio de diez repeticiones.

No presentamos los tiempos para el cálculo de la cercanía del Grafo 3 porque tomaría días. Para los mil primeros nodos, se obtuvo un tiempo de ejecución de \num{261.9417} segundos. De aquí, se deduce que la evaluación total demoraría por lo menos cuatro días.

Para estudiar PageRank, usamos el algoritmo en los tres grafos bajo diferentes precisiones \(\epsilon\). En la Figura~\ref{fig:itpr} se tiene la cantidad de iteraciones requeridas en cada caso, con \(\alpha = 0.85\). Se hacen dos observaciones. A simple vista, se ve que en los dos primeros casos se sigue un modelo log-lineal, y que esto no ocurre en el Grafo 3. En segundo lugar, nótese que la cantidad de iteraciones en el Grafo 1 siempre es mayor a lo requerido en el Grafo 2, a pesar de tener menos vértices. Al hacerse una regresión, se tendrían dos líneas paralelas. Sin embargo, las iteraciones requeridas en el Grafo 3 son mayores para precisiones muy elevadas (\(\epsilon \leq 10^{-12} \)), y menores en el otro extremo.

Una manera de ver si los resultados de \textproc{PageRank} tienen sentido es calculando la suma de los valores para todos los vértices. Dada la interpretación probabilística de esta métrica, el resultado debe ser 1. En efecto, esto se cumple con cada vez más precisión a medida que \(\epsilon\) se hace más pequeño. Para \(\epsilon\) relativamente grande, se puede observar en la Figura~\ref{fig:itpr} que se detecta convergencia en la primera iteración, sin embargo, en estos casos se consiguieron sumas mucho menores que 1.

\begin{figure}
\centering
\begin{tikzpicture}% table
\begin{semilogxaxis} [
	xlabel=Precisión \(\epsilon\),
	legend style={font=\tiny},
	log basis x = {10},
	xtickten = {-14,-12,...,-6, -4},
	ylabel=Iteraciones,
	only marks,
	cycle list name=color,
]
\addplot table {preps1.dat};
\addlegendentry{Grafo 1}
\addplot table {preps2.dat};
\addlegendentry{Grafo 2}
\addplot[only marks] table {preps3.dat};
\addlegendentry{Grafo 3}
\addplot[sharp plot] [
domain=1e-14:1e-4,
samples=100,
color=blue,
]
{-16.22-7.236*ln(x)/ln(10)};
\addplot[sharp plot] [
domain=1e-14:1e-4,
samples=100,
color=red,
]
{-20.98-7.261*ln(x)/ln(10)};
\end{semilogxaxis}
\end{tikzpicture}
\caption{Relación entre precisión pedida \(\epsilon\) y la cantidad de iteraciones necesarias para llegar a ella en \textproc{PageRank}. La regresión para el Grafo 1 resulta en la ecuación $y = -16.22-7.24\log_{10}(x)$, con coeficiente de determinación  $R^2 = 0.9999$; análogamente para el Grafo 2: $y = -20.98-7.26\log_{10}(x)$, $R^2 = 0.9998$.}
\label{fig:itpr}
\end{figure}

Al ver esta gráfica, es natural preguntarse qué pasaría si se hace variar \(\alpha\). Esto es lo que se muestra en la Figura~\ref{fig:3dpr}.

En el Cuadro~\ref{tab:erdos}, se colocan los diez autores más importantes según cada métrica. Los valores de \(C_C\) y \(C_{PR}\) han sido normalizados de las maneras mostradas en la primera fila. Se han resaltado los nombres de las personas que aparecen en más de una lista.

\begin{figure}
\centering
\begin{tikzpicture}
\begin{axis} [
xlabel=Precisión \(\epsilon\),
ylabel=\(\alpha\),
zlabel=Iteraciones,
cycle list name=color,
]
\addplot3[surf] table {3dplot2.dat};
\end{axis}
\end{tikzpicture}
\caption{Superficie de las iteraciones al variar la precisión \(\epsilon\) y el factor de amortiguamiento \(\alpha\) para el Grafo 2. El Grafo 1 muestra un comportamiento similar.}
\label{fig:3dpr}
\end{figure}

\begin{table*}
	\renewcommand{\arraystretch}{1.3}
	\caption{Los 10 autores más importantes según cada métrica}
	\label{tab:erdos}
	\centering
	\begin{tabular}{c|c|c|c}
		\hline
		Grafo & \(C_D\) & \(C_C (|V| - 1)\) & \(C_{PR} (|V|)\)\\
		\hline\hline
		1 & \textbf{David Maier} (592)& \textbf{David Maier} (1.3140)& \textbf{David Maier} (16.4470)\\
		  &	\textbf{Divesh Srivastava} (487)& \textbf{Dan Suciu} (1.3137)& Lucila Ohno-Machado (11.5580)\\
		  &	Paola Mello (450)& Jeffrey D. Ullman (1.3070)& \textbf{Divesh Srivastava} (10.4953)\\
		  &	Giuseppe De Giacomo (408)& Alon Y. Halevy (1.3012)& Richard Hull (9.1032)\\
		  &	\textbf{Serge Abiteboul} (402)& Joseph M. Hellerstein (1.2974)& \textbf{Bertram Ludascher} (8.8004)\\
		  & \textbf{Bertram Ludascher} (387)& Michael J. Franklin (1.2959)& \textbf{Serge Abiteboul} (8.099)\\
		  & Diego Calvanese (382)& Magdalena Balazinska (1.2946)& George Varghese (7.4728)\\
		  & Evelina Lamma (342)& Mary F. Fernandez (1.2832)& K. K. Ramakrishnan (7.3172)\\
		  & \textbf{Dan Suciu} (334)& \textbf{Raghu Ramakrishnan} (1.2770)& \textbf{Dan Suciu} (7.1371)\\
		  &	\textbf{Raghu Ramakrishnan} (326)& Lois M. L. Delcambre (1.2765)& Wenfei Fan (6.6365)\\
		\hline
		2 & \textbf{Xiang Li} (1054)& \textbf{Jiawei Han} (1.2943)& \textbf{Xiang Li} (35.1669)\\
		  &	\textbf{Carole A. Goble} (968)& \textbf{Philip S. Yu} (1.2858)& Liang Chen (20.7484)\\
		  & \textbf{John Mylopoulos} (847)& Xifeng Yan (1.2712)& Lei Shi (19.9949)\\
		  & \textbf{Jiawei Han} (801)& Kun-Lung Wu (1.2692)& Lin Li (19.1619)\\
		  &	\textbf{Philip S. Yu} (786)& Charu C. Aggarwal (1.2686)& Ping Li (18.4318)\\
		  & \textbf{Hector Garcia-Molina} (781)& Jian Pei (1.2651)& \textbf{John Mylopoulos} (18.3041)\\
		  & Gerhard Weikum (733)& Haixun Wang (1.2572)& \textbf{Hector Garcia-Molina} (18.0143)\\
		  & Yuan Chen (728)& Laks V. S. Lakshmanan (1.2537)& \textbf{Carole A. Goble} (17.7333)\\
		  & Ewan Birney (701)& Divesh Srivastava (1.2520)& \textbf{David Maier} (17.4873)\\
		  & \textbf{David Maier} (690)& Tarek F. Abdelzaher (1.2504)& Fei Wang (17.3025)\\
		\hline
		3 & \textbf{Wen Gao} (2928)& & \textbf{Wei Wang} (120.0663)\\
		  &	\textbf{Wei Wang} (2549)& & \textbf{Wei Liu} (107.5916)\\
		  &	\textbf{Wei Liu} (2529)& & \textbf{Yan Zhang} (103.8319)\\
		  & \textbf{Yan Zhang} (2429)& & \textbf{Wen Gao} (94.8491)\\
		  &	H. Vincent Poor (2356)& & Lei Wang (94.6526)\\
		  & \textbf{Xin Li} (2221)& & \textbf{Xin Li} (93.3889)\\
		  & Philip S. Yu (2094)& & \textbf{Jing Li} (89.8887)\\
		  & Jiawei Han (2069)& & Wei Li (89.8564)\\
		  & Thomas S. Huang (2037)& & Wei Zhang (87.2491)\\
		  &	\textbf{Jing Li} (2027)& & Li Zhang (86.1994)\\
		  
		\hline
	\end{tabular}
\end{table*}
\section{Discusión}
La métrica del grado mide localmente la importancia de un nodo. Es bastante fácil y rápida de calcular, pero no permite capturar la influencia a más de un nivel, siendo susceptible a sobreestimar el valor de nodos en conglomerados. Sin embargo, puede ser una buena aproximación. Como se ve en el Cuadro~\ref{tab:erdos}, en promedio cinco autores aparecen tanto en la lista de \(C_D\) como en la de \(C_{PR}\), siendo el número exacto mayor al aumentar el tamaño del grafo.

La métrica de cercanía permite capturar de manera más global la estructura del grafo. De los autores más imporantes, son relativamente pocos los que coinciden con las otras dos métricas, demostrando que es un criterio substancialmente diferente. El problema con esta métrica es que el cálculo es bastante lento. Sin embargo, a diferencia de PageRank, es paralelizable al ser cada nodo independiente del resto. El método de hacer el cálculo de forma paralela es escalable. Así, es posible emplear procesadores con más núcleos para hacer el mismo trabajo en fracciones de los tiempos originales.

Hay otras alternativas para este procedimiento. Se puede emplear una GPU para tener acceso a más núcleos. La desventaja de este método es que la programación es más complicada, pues requiere manejar memoria adicionalmente en el GPU. Es posible, también, que el tiempo requerido para este manejo de memoria adicional contrarreste los beneficios de la mayor cantidad de núcleos. Otra alternativa es emplear un grid de computadoras, a través algún sistema de pase de mensajes como \pkg{Message Passing Interface} (\pkg{MPI}). Si los procesadores poseen más de un núcleo, es posible usar este sistema en conjunto con \pkg{OpenMP}. El costo de acceso a varias computadoras, sin embargo, puede resultar ser un inconveniente.

La métrica de PageRank presenta las ventajas de tener una interpretación intuitiva, y de no capturar sólo la estructura local del grafo, además de ser rápidamente calculable bajo los parámetros usuales. A través de la Figura~\ref{fig:3dpr}, se puede concluir experimentalmente que tanto el aumento de precisión, como el acercamiento de \(\alpha\) a 1, aumenta la cantidad de iteraciones necesarias para PageRank. Este resultado es una consecuencia de resultados teóricos encontrados en la literatura de análisis de sensibilidad de este algoritmo\cite{stanford}.
\section{Conclusiones}
En este artículo se describieron y analizaron tres métricas de centralidad para grafos no dirigidos con pesos, con sus respectivos algoritmos, aplicados en particular a una red de coautoría. Se compararon los tiempos de ejecución para todos los algoritmos. También se tomaron en cuenta los diez autores más importantes identificados por cada uno. La cantidad de iteraciones para PageRank fue estudiada bajo diferentes precisiones y factores de amortiguamiento, llegando a un modelo log-lineal para el primer parámetro. Se hizo énfasis en la escalabilidad de la paralelización para la métrica de cercanía.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi

\IEEEtriggeratref{8}


\begin{thebibliography}{2}

\bibitem{brandes}
U.~Brandes \textit{et al.}, \emph{Network Analysis: Methodological Foundations}.\hskip 1em plus
0.5em minus 0.4em\relax Saarland, Alemania: Springer, 2005, pp. 16-34.

\bibitem{bondy}
J.~Bondy y U.~Murty, \emph{Graph Theory}. Londres, Inglaterra: Springer, 2008.

\bibitem{diestel}
R.~Diestel, \emph{Graph Theory}, 3ra edición. Heidelberg, Alemania: Springer, 2005.

\bibitem{bollobas}
B.~Bollobás, \emph{Modern Graph Theory}. New York City, NY: Springer, 1998.

\bibitem{google}
S.~Brin y L.~Page, ``The Anatomy of a Large-Scale Hypertextual Web Search Engine''. \emph{Computer Networks and ISDN Systems}, vol. 30, pp. 107–117, Abril 1998.

\bibitem{langville}
A.~Langville, C.~Meyer, \emph{Google's PageRank and Beyond: The Science of Search Engine Rankings}. Princeton, NJ: Princeton University Press, 2012.

\bibitem{newman}
M.~Newman, ``Scientific collaboration networks. II\@. Shortest paths, weighted networks, and centrality''. \emph{Physical Review}, vol. 64, Junio 2001.

\bibitem{sinha}
R.~Sinha y R.~Mihalcea, ``Unsupervised Graph-based Word Sense Disambiguation Using Measures of Word Semantic Similarity'' en \emph{Proceedings of the International Conference on Semantic Computing}, Washington, DC, 2007, pp. 363-369.

\bibitem{heap}
M.~Chen \textit{et al.}, ``Priority Queues and Dijkstra’s Algorithm,'' The University of Texas at Austin, Department of Computer Sciences, Austin, TX, Rep. TR-07-54, Oct. 2007.

\bibitem{clrs}
T.~Cormen \textit{et al.} \emph{Introduction to Algorithms}. Cambridge, MA: MIT Press, 2009.

\bibitem{rochat}
Y.~Yannick, ``Closeness Centrality Extended To Unconnected Graphs: The Harmonic Centrality Index,'' University of Lausanne, Institute of Applied Mathematics, Suiza, 2009.

\bibitem{meyer}
C.~Meyer. \emph{Matrix analysis and applied linear algebra}. Philadelphia, PA: Society for Industrial and Applied Mathematics
, 2000.

\bibitem{openmp}
OpenMP Architecture Review Board. \emph{OpenMP Application Program Interface}, versión 4.0. Julio 2013.

\bibitem{stanford}
D.~Gleich, ``Models and Algorithms for PageRank Sensitivity'', Tesis de Doctorado, Institute of Computational and Mathematical Engineering, Stanford Univ., San Francisco, CA, 2009.

\end{thebibliography}

\end{document}